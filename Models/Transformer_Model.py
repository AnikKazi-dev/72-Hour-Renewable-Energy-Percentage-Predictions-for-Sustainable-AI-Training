# Auto-generated from notebook by scripts/convert_notebooks.py
# Do not edit this file directly; edit the corresponding .ipynb instead.

#!/usr/bin/env python
# coding: utf-8

# # 72-Hour Renewable Energy Forecast (Transformer) - Season-aware Model with Benchmarking

# In[1]:


import pandas as pd


import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D
from datetime import datetime, timedelta
import os
from sklearn.metrics import r2_score

# Added: project helpers for results and paths
try:
    from scripts.results_manager import ResultsManager
    from scripts.metrics import as_dict as metrics_as_dict
    from scripts.paths import DATA_DIR
except Exception:
    ResultsManager = None  # type: ignore
    metrics_as_dict = None  # type: ignore
    DATA_DIR = None  # type: ignore

print(f"TensorFlow Version: {tf.__version__}")


# In[2]:


from scripts.season import resolve_season, months_for, data_path
SEASON = resolve_season(default='Winter').capitalize()
MONTHS = months_for(SEASON.lower())

# --- Data Parameters ---
LOOK_BACK = 72          # Use past 72 hours (3 days) of data to predict
FORECAST_HORIZON = 72   # Predict next 72 hours
N_FEATURES = 1          # We are using only 'renewable_percentage' as the input feature
COUNTRY_CODE = (os.environ.get("COUNTRY_CODE") or os.environ.get("BIONET_COUNTRY_CODE") or "DE").upper()
YEARS_HISTORY = 5       # Set history to 5 years

# --- Seasonal Parameters (Winter) ---
DATA_FILENAME = str(data_path(f"energy_data_{COUNTRY_CODE}_{YEARS_HISTORY}years_{SEASON.lower()}.csv"))


# In[3]:


def create_sequences(data_values_scaled, look_back, forecast_horizon):
    """Creates sequences of X (input) and y (target) for time series forecasting."""
    X_list, y_list = [], []
    if len(data_values_scaled) < look_back + forecast_horizon:
        print(f"Not enough data to create sequences. Data length: {len(data_values_scaled)}, "
              f"Required: {look_back + forecast_horizon}")
        return np.array(X_list), np.array(y_list)
        
    for i in range(len(data_values_scaled) - look_back - forecast_horizon + 1):
        X_list.append(data_values_scaled[i:(i + look_back)])
        y_list.append(data_values_scaled[(i + look_back):(i + look_back + forecast_horizon)])
    return np.array(X_list), np.array(y_list)


# In[4]:


# --- Load and filter data from the seasonal CSV file ---
print(f"Loading data from file: {DATA_FILENAME}")
cached_data = None
try:
    # First try current working directory
    cached_data = pd.read_csv(DATA_FILENAME, index_col=0, parse_dates=True)
except FileNotFoundError:
    # Fallback to project Data/ folder if available
    if DATA_DIR is not None:
        alt = (DATA_DIR / DATA_FILENAME) if hasattr(DATA_DIR, 'joinpath') else None
        if alt is not None and alt.exists():
            print(f"Falling back to Data folder: {alt}")
            cached_data = pd.read_csv(alt, index_col=0, parse_dates=True)
    if cached_data is None:
        print(f"CRITICAL: Data file not found at '{os.path.abspath(DATA_FILENAME)}'.")
        print(f"Please ensure the {SEASON.lower()} CSV file exists in the project Data/ folder.")
        renewable_series_data = None
    else:
        # proceed with cached_data below
        pass
if cached_data is not None:
    # Filter the loaded data to ensure it only contains selected season months
    seasonal_data = cached_data[cached_data.index.month.isin(MONTHS)]
    renewable_series_data = seasonal_data.squeeze()
    print(f"{SEASON} data loaded and filtered successfully.")

# --- The rest of the cell proceeds from here ---
X_train, y_train = np.array([]), np.array([])
X_valid, y_valid = np.array([]), np.array([])
X_test, y_test = np.array([]), np.array([])
scaler = MinMaxScaler(feature_range=(0, 1)) # Initialize scaler here for later use

if renewable_series_data is not None and not renewable_series_data.empty:
    print(f"\nOriginal data points loaded: {len(renewable_series_data)}")
    # Scale the data
    data_for_scaling = renewable_series_data.values.reshape(-1, 1)
    scaled_data_values = scaler.fit_transform(data_for_scaling).flatten()

    # Create sequences
    X_seq, y_seq = create_sequences(scaled_data_values, LOOK_BACK, FORECAST_HORIZON)

    if X_seq.shape[0] > 0:
        X_seq = X_seq.reshape((X_seq.shape[0], X_seq.shape[1], N_FEATURES))

        # Chronological split for time series
        train_size_idx = int(len(X_seq) * 0.70)
        valid_size_idx = int(len(X_seq) * 0.15)

        X_train, y_train = X_seq[:train_size_idx], y_seq[:train_size_idx]
        X_valid, y_valid = X_seq[train_size_idx : train_size_idx + valid_size_idx], y_seq[train_size_idx : train_size_idx + valid_size_idx]
        X_test, y_test = X_seq[train_size_idx + valid_size_idx:], y_seq[train_size_idx + valid_size_idx:]
else:
    print("CRITICAL: No data loaded or data is empty. Cannot proceed with model training.")

print(f"\nData Split:")
print(f"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")
print(f"X_valid shape: {X_valid.shape}, y_valid shape: {y_valid.shape}")
print(f"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}")


# In[5]:


def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
    # Attention and Normalization
    x = LayerNormalization(epsilon=1e-6)(inputs)
    x = MultiHeadAttention(
        key_dim=head_size, num_heads=num_heads, dropout=dropout
    )(x, x)
    x = Dropout(dropout)(x)
    res = x + inputs

    # Feed Forward Part
    x = LayerNormalization(epsilon=1e-6)(res)
    x = Dense(ff_dim, activation="relu")(x)
    x = Dropout(dropout)(x)
    x = Dense(inputs.shape[-1])(x)
    return x + res

def build_transformer_model(
    input_shape,
    head_size,
    num_heads,
    ff_dim,
    num_transformer_blocks,
    mlp_units,
    dropout=0,
    mlp_dropout=0,
):
    inputs = Input(shape=input_shape)
    x = inputs
    for _ in range(num_transformer_blocks):
        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)

    x = GlobalAveragePooling1D(data_format="channels_first")(x)
    for dim in mlp_units:
        x = Dense(dim, activation="relu")(x)
        x = Dropout(mlp_dropout)(x)
    outputs = Dense(FORECAST_HORIZON)(x)
    model = Model(inputs, outputs)
    model.summary(line_length=120)
    return model

# Build the model
solar_model = None
run_name = os.environ.get('RUN_NAME', f"transformer_{SEASON.lower()}")
rm = ResultsManager(run_name) if ResultsManager else None
if X_train.shape[0] > 0:
    solar_model = build_transformer_model(
        input_shape=(LOOK_BACK, N_FEATURES),
        head_size=256,
        num_heads=4,
        ff_dim=4,
        num_transformer_blocks=4,
        mlp_units=[128],
        mlp_dropout=0.4,
        dropout=0.25,
    )
else:
    print("Skipping model building as no training data is available.")

    import os  # Ensure os is imported for environment variable access


    COUNTRY_CODE = (os.environ.get("COUNTRY_CODE") or os.environ.get("BIONET_COUNTRY_CODE") or "DE").upper()
# In[ ]:


history = None
if solar_model and X_train.shape[0] > 0:
    print("\n--- Starting Model Training ---")
    
    solar_model.compile(optimizer='adam', loss='mean_absolute_error', metrics=['mae'])
    
    early_stopping = tf.keras.callbacks.EarlyStopping(
        monitor='val_loss', 
        patience=30,
        restore_best_weights=True
    )
    
    history = solar_model.fit(X_train, y_train,
                              validation_data=(X_valid, y_valid),
                              epochs=500,
                              batch_size=32,
                              verbose=1,
                              callbacks=[early_stopping])

    # --- Plotting and Evaluation ---
    from sklearn.metrics import mean_absolute_error, mean_squared_error

    # Plotting Training Loss
    plt.figure(figsize=(10, 6))
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title(f'Model Loss During Training ({SEASON}) - Transformer')
    plt.ylabel('Loss (Mean Absolute Error)')
    plt.xlabel('Epoch')
    plt.legend(loc='upper right')
    plt.grid(True)
    # Save training loss plot into Results/images
    if rm:
        plt.savefig(rm.image_path(f"training_loss_plot_{SEASON.lower()}_transformer.png"))
    else:
        plt.savefig(f"training_loss_plot_{SEASON.lower()}_transformer.png")

    # Final Model Evaluation
    if X_test.shape[0] > 0:
        print("\n--- Evaluating Model on Test Set ---")
        y_pred_scaled = solar_model.predict(X_test)
        
        y_test_reshaped = y_test.reshape(-1, 1)
        y_pred_reshaped = y_pred_scaled.reshape(-1, 1)

        y_test_inversed = scaler.inverse_transform(y_test_reshaped)
        y_pred_inversed = scaler.inverse_transform(y_pred_reshaped)

        mae_overall = mean_absolute_error(y_test_inversed, y_pred_inversed)
        mse_overall = mean_squared_error(y_test_inversed, y_pred_inversed)
        rmse_overall = np.sqrt(mse_overall)
        r2_overall = r2_score(y_test_inversed, y_pred_inversed)

        print(f"\nOverall Test Set Metrics (on inverse-transformed data):")
        print(f"  Mean Absolute Error (MAE): {mae_overall:.4f}")
        print(f"  Mean Squared Error (MSE):  {mse_overall:.4f}")
        print(f"  Root Mean Squared Error (RMSE): {rmse_overall:.4f}")
        print(f"  R-squared (R²): {r2_overall:.4f}")

        # Make a Sample Prediction and Plot
        num_plots = min(3, len(X_test))
        if num_plots > 0:
            plt.figure(figsize=(15, 5 * num_plots))
            for i in range(num_plots):
                sample_index = np.random.randint(0, len(X_test))
                
                historical_input_inversed = scaler.inverse_transform(X_test[sample_index])
                y_true_inversed_plot = scaler.inverse_transform(y_test[sample_index].reshape(-1, 1))
                y_pred_inversed_plot = scaler.inverse_transform(y_pred_scaled[sample_index].reshape(-1, 1))

                time_axis_input = np.arange(-LOOK_BACK, 0)
                time_axis_output = np.arange(0, FORECAST_HORIZON)
                
                plt.subplot(num_plots, 1, i + 1)
                plt.plot(time_axis_input, historical_input_inversed.flatten(), label='Historical Input', marker='o', linestyle=':', color='gray', alpha=0.7)
                plt.plot(time_axis_output, y_true_inversed_plot, label='Actual Future', marker='.', color='blue')
                plt.plot(time_axis_output, y_pred_inversed_plot, label='Predicted Future', marker='x', linestyle='--', color='red')
                plt.title(f'{FORECAST_HORIZON}-Hour Forecast ({SEASON} - Test Sample {sample_index})')
                plt.xlabel('Time (Hours into the future)')
                plt.ylabel('Renewable Percentage (%)')
                plt.axvline(x=0, color='k', linestyle='--', linewidth=0.8, label='Forecast Start (T=0)')
                plt.legend()
                plt.grid(True)
            plt.tight_layout()
            if rm:
                plt.savefig(rm.image_path(f"forecast_examples_{SEASON.lower()}_transformer.png"))
            else:
                plt.savefig(f"forecast_examples_{SEASON.lower()}_transformer.png")
            
        # --- BENCHMARKING REPORT ---
        print("\n\n--- BENCHMARKING REPORT ---")
        total_params = solar_model.count_params()
        model_size_mb = total_params * 4 / (1024**2)
        print(f"{'Model Name:':<25} Transformer ({SEASON})")
        print(f"{'Key Features / Inputs:':<25} Renewable % (Univariate)")
        print("-" * 40)
        print(f"{'MAE:':<25} {mae_overall:.4f}")
        print(f"{'RMSE:':<25} {rmse_overall:.4f}")
        print(f"{'R²:':<25} {r2_overall:.4f}")
        print(f"{'Model Size (MB):':<25} {model_size_mb:.2f} MB")
        print(f"{'# of Params (M):':<25} {total_params / 1e6:.2f} M")
        print("-" * 40)
        # Save metrics and a compact report
        if rm and metrics_as_dict:
            try:
                metrics_payload = metrics_as_dict(y_test_inversed.flatten(), y_pred_inversed.flatten())
                # include r2 as well
                metrics_payload["r2"] = float(r2_overall)
                rm.save_metrics(metrics_payload)
                rm.append_report({
                    "model": "Transformer",
                    "season": SEASON,
                    "params": {
                        "look_back": LOOK_BACK,
                        "horizon": FORECAST_HORIZON,
                        "features": N_FEATURES,
                        "total_params": int(total_params),
                    },
                    "metrics": metrics_payload,
                })
            except Exception:
                pass
    else:
        print("No test data to evaluate or plot.")
else:
    print("CRITICAL: Not enough data to train the model.")
# Auto-generated from notebook by scripts/convert_notebooks.py
# Do not edit this file directly; edit the corresponding .ipynb instead.

#!/usr/bin/env python
# coding: utf-8

# # 72-Hour Renewable Energy Forecast (PatchTST) - V3 Improved Winter Model
# 
# ## Modifications for V3:
# 
# This notebook enhances the V2 PatchTST model with several key improvements aimed at boosting predictive accuracy and robustness:
# 
# 1.  **Huber Loss Function**: The `Mean Absolute Error` loss function has been replaced with the `Huber` loss. This provides a more robust error metric that is less sensitive to outliers, which is beneficial for noisy time series data.
# 
# 2.  **L2 Regularization**: To prevent overfitting, L2 regularization (`kernel_regularizer`) has been added to all `MultiHeadAttention`, `Conv1D`, and `Dense` layers. This encourages the model to learn smaller, more generalizable weights.
# 
# 3.  **Increased Model Capacity**: The model's capacity has been significantly increased to capture more complex patterns in the data. The number of encoder layers is increased from 2 to 3, the number of attention heads from 2 to 4, and the feed-forward dimension (`ff_dim`) from 128 to 256.
# 
# 4.  **Deeper MLP Head**: The final MLP head has been made deeper (from a single `Dense(256)` layer to `Dense(512)` -> `Dense(256)`). This allows for more sophisticated feature processing before the final output, potentially improving forecast accuracy.

# In[ ]:


import pandas as pd


import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, Add, GlobalAveragePooling1D, Conv1D, Reshape, Permute, Embedding
from tensorflow.keras.callbacks import ReduceLROnPlateau
from datetime import datetime, timedelta
import os
from sklearn.metrics import r2_score
from scripts.season import resolve_season, months_for, data_path
SEASON = resolve_season(default='Winter').capitalize()
MONTHS = months_for(SEASON.lower())

print(f"TensorFlow Version: {tf.__version__}")


# In[ ]:


# --- Data Parameters ---
LOOK_BACK = 72          # Use past 72 hours (3 days) of data to predict
FORECAST_HORIZON = 72   # Predict next 72 hours
N_FEATURES = 1          # We are using only 'renewable_percentage' as the input feature
COUNTRY_CODE = (os.environ.get("COUNTRY_CODE") or os.environ.get("BIONET_COUNTRY_CODE") or "DE").upper()
YEARS_HISTORY = 5       # Set history to 5 years

# --- Seasonal Parameters (Winter) ---
DATA_FILENAME = str(data_path(f"energy_data_{COUNTRY_CODE}_{YEARS_HISTORY}years_{SEASON.lower()}.csv"))


# In[ ]:


def create_sequences(data_values_scaled, look_back, forecast_horizon):
    """Creates sequences of X (input) and y (target) for time series forecasting."""
    X_list, y_list = [], []
    if len(data_values_scaled) < look_back + forecast_horizon:
        print(f"Not enough data to create sequences. Data length: {len(data_values_scaled)}, "
              f"Required: {look_back + forecast_horizon}")
        return np.array(X_list), np.array(y_list)
        
    for i in range(len(data_values_scaled) - look_back - forecast_horizon + 1):
        X_list.append(data_values_scaled[i:(i + look_back)])
        y_list.append(data_values_scaled[(i + look_back):(i + look_back + forecast_horizon)])
    return np.array(X_list), np.array(y_list)


# In[ ]:


# --- Load and filter data from the seasonal CSV file ---
print(f"Loading data from file: {DATA_FILENAME}")
try:
    cached_data = pd.read_csv(DATA_FILENAME, index_col=0, parse_dates=True)
    # Filter the loaded data to ensure it only contains selected season months
    seasonal_data = cached_data[cached_data.index.month.isin(MONTHS)]
    renewable_series_data = seasonal_data.squeeze()
    print(f"{SEASON} data loaded and filtered successfully.")
except FileNotFoundError:
    print(f"CRITICAL: Data file not found at '{os.path.abspath(DATA_FILENAME)}'.")
    print(f"Please ensure the {SEASON.lower()} CSV file exists.")
    renewable_series_data = None

# --- The rest of the cell proceeds from here ---
X_train, y_train = np.array([]), np.array([])
X_valid, y_valid = np.array([]), np.array([])
X_test, y_test = np.array([]), np.array([])
scaler = MinMaxScaler(feature_range=(0, 1)) # Initialize scaler here for later use

if renewable_series_data is not None and not renewable_series_data.empty:
    print(f"\nOriginal data points loaded: {len(renewable_series_data)}")
    # Scale the data
    data_for_scaling = renewable_series_data.values.reshape(-1, 1)
    scaled_data_values = scaler.fit_transform(data_for_scaling).flatten()

    # Create sequences
    X_seq, y_seq = create_sequences(scaled_data_values, LOOK_BACK, FORECAST_HORIZON)

    if X_seq.shape[0] > 0:
        X_seq = X_seq.reshape((X_seq.shape[0], X_seq.shape[1], N_FEATURES))

        # Chronological split for time series
        train_size_idx = int(len(X_seq) * 0.70)
        valid_size_idx = int(len(X_seq) * 0.15)

        X_train, y_train = X_seq[:train_size_idx], y_seq[:train_size_idx]
        X_valid, y_valid = X_seq[train_size_idx : train_size_idx + valid_size_idx], y_seq[train_size_idx : train_size_idx + valid_size_idx]
        X_test, y_test = X_seq[train_size_idx + valid_size_idx:], y_seq[train_size_idx + valid_size_idx:]
else:
    print("CRITICAL: No data loaded or data is empty. Cannot proceed with model training.")

print(f"\nData Split:")
print(f"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")
print(f"X_valid shape: {X_valid.shape}, y_valid shape: {y_valid.shape}")
print(f"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}")


# In[ ]:


# V2 Improvement: Add Positional Embedding
class TokenAndPositionEmbedding(tf.keras.layers.Layer):
    def __init__(self, maxlen, embed_dim):
        super(TokenAndPositionEmbedding, self).__init__()
        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)
        self.dense_proj = Dense(embed_dim)

    def call(self, x):
        maxlen = tf.shape(x)[-2]
        positions = tf.range(start=0, limit=maxlen, delta=1)
        positions = self.pos_emb(positions)
        x = self.dense_proj(x)
        return x + positions

# V2: Enhanced Patching Layer
class Patching(tf.keras.layers.Layer):
    def __init__(self, patch_size, **kwargs):
        super(Patching, self).__init__(**kwargs)
        self.patch_size = patch_size

    def call(self, inputs):
        batch_size = tf.shape(inputs)[0]
        patches = tf.image.extract_patches(
            images=tf.expand_dims(inputs, -1),
            sizes=[1, self.patch_size, 1, 1],
            strides=[1, self.patch_size, 1, 1],
            rates=[1, 1, 1, 1],
            padding="VALID",
        )
        patch_dims = patches.shape[-1]
        patches = tf.reshape(patches, [batch_size, -1, patch_dims])
        return patches

def patchtst_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
    l2_reg = tf.keras.regularizers.l2(0.001)
    # Attention and Normalization
    x = LayerNormalization(epsilon=1e-6)(inputs)
    x = MultiHeadAttention(
        key_dim=head_size, num_heads=num_heads, dropout=dropout, kernel_regularizer=l2_reg # V3: L2 reg
    )(x, x)
    x = Dropout(dropout)(x)
    res = x + inputs

    # Feed Forward Part
    x = LayerNormalization(epsilon=1e-6)(res)
    x = Conv1D(filters=ff_dim, kernel_size=1, activation="relu", kernel_regularizer=l2_reg)(x) # V3: L2 reg
    x = Dropout(dropout)(x)
    x = Conv1D(filters=inputs.shape[-1], kernel_size=1, kernel_regularizer=l2_reg)(x) # V3: L2 reg
    return x + res

def build_patchtst_model(
    input_shape,
    patch_size,
    head_size,
    num_heads,
    ff_dim,
    num_encoder_layers,
    mlp_units,
    dropout=0,
    mlp_dropout=0,
):
    l2_reg = tf.keras.regularizers.l2(0.001)
    inputs = Input(shape=input_shape)
    
    # Patching
    patches = Patching(patch_size)(inputs)
    num_patches = input_shape[0] // patch_size
    
    # Encoder
    x = TokenAndPositionEmbedding(num_patches, head_size)(patches)
    for _ in range(num_encoder_layers):
        x = patchtst_encoder(x, head_size, num_heads, ff_dim, dropout)

    x = GlobalAveragePooling1D(data_format="channels_last")(x)
    # V3: Deeper MLP Head
    for dim in mlp_units:
        x = Dense(dim, activation="relu", kernel_regularizer=l2_reg)(x)
        x = Dropout(mlp_dropout)(x)
    outputs = Dense(FORECAST_HORIZON)(x)
    model = Model(inputs, outputs)
    model.summary(line_length=120)
    return model

# Build the model
solar_model = None
if X_train.shape[0] > 0:
    solar_model = build_patchtst_model(
        input_shape=(LOOK_BACK, N_FEATURES),
        patch_size=12,
        head_size=64, 
        num_heads=4,   # V3: Increased capacity
        ff_dim=256,    # V3: Increased capacity
        num_encoder_layers=3, # V3: Increased capacity
        mlp_units=[512, 256], # V3: Deeper MLP head
        mlp_dropout=0.3,
        dropout=0.2,
    )
else:
    print("Skipping model building as no training data is available.")


# In[ ]:


history = None
if solar_model and X_train.shape[0] > 0:
    print("\n--- Starting Model Training ---")
    
    # V3: Use Huber loss
    loss = tf.keras.losses.Huber()
    solar_model.compile(optimizer='adam', loss=loss, metrics=['mae'])
    
    early_stopping = tf.keras.callbacks.EarlyStopping(
        monitor='val_loss', 
        patience=30,
        restore_best_weights=True
    )
    
    # V2 Improvement: Add learning rate scheduler
    lr_scheduler = ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.2,
        patience=5,
        min_lr=1e-6,
        verbose=1
    )
    
    history = solar_model.fit(X_train, y_train,
                              validation_data=(X_valid, y_valid),
                              epochs=500,
                              batch_size=32,
                              verbose=1,
                              callbacks=[early_stopping, lr_scheduler])

    # --- Plotting and Evaluation ---
    from sklearn.metrics import mean_absolute_error, mean_squared_error

    # Plotting Training Loss
    plt.figure(figsize=(10, 6))
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title(f'Model Loss During Training ({SEASON}) - PatchTST V3')
    plt.ylabel('Loss (Huber)')
    plt.xlabel('Epoch')
    plt.legend(loc='upper right')
    plt.grid(True)
    plt.savefig(f"training_loss_plot_{SEASON.lower()}_patchtst_v3.png")

    # Final Model Evaluation
    if X_test.shape[0] > 0:
        print("\n--- Evaluating Model on Test Set ---")
        y_pred_scaled = solar_model.predict(X_test)
        
        y_test_reshaped = y_test.reshape(-1, 1)
        y_pred_reshaped = y_pred_scaled.reshape(-1, 1)

        y_test_inversed = scaler.inverse_transform(y_test_reshaped)
        y_pred_inversed = scaler.inverse_transform(y_pred_reshaped)

        mae_overall = mean_absolute_error(y_test_inversed, y_pred_inversed)
        mse_overall = mean_squared_error(y_test_inversed, y_pred_inversed)
        rmse_overall = np.sqrt(mse_overall)
        r2_overall = r2_score(y_test_inversed, y_pred_inversed)

        print(f"\nOverall Test Set Metrics (on inverse-transformed data):")
        print(f"  Mean Absolute Error (MAE): {mae_overall:.4f}")
        print(f"  Mean Squared Error (MSE):  {mse_overall:.4f}")
        print(f"  Root Mean Squared Error (RMSE): {rmse_overall:.4f}")
        print(f"  R-squared (R²): {r2_overall:.4f}")

        # Make a Sample Prediction and Plot
        num_plots = min(3, len(X_test))
        if num_plots > 0:
            plt.figure(figsize=(15, 5 * num_plots))
            for i in range(num_plots):
                sample_index = np.random.randint(0, len(X_test))
                
                historical_input_inversed = scaler.inverse_transform(X_test[sample_index])
                y_true_inversed_plot = scaler.inverse_transform(y_test[sample_index].reshape(-1, 1))
                y_pred_inversed_plot = scaler.inverse_transform(y_pred_scaled[sample_index].reshape(-1, 1))

                time_axis_input = np.arange(-LOOK_BACK, 0)
                time_axis_output = np.arange(0, FORECAST_HORIZON)
                
                plt.subplot(num_plots, 1, i + 1)
                plt.plot(time_axis_input, historical_input_inversed.flatten(), label='Historical Input', marker='o', linestyle=':', color='gray', alpha=0.7)
                plt.plot(time_axis_output, y_true_inversed_plot, label='Actual Future', marker='.', color='blue')
                plt.plot(time_axis_output, y_pred_inversed_plot, label='Predicted Future', marker='x', linestyle='--', color='red')
                plt.title(f'{FORECAST_HORIZON}-Hour Forecast ({SEASON} - Test Sample {sample_index}) - V3')
                plt.xlabel('Time (Hours into the future)')
                plt.ylabel('Renewable Percentage (%)')
                plt.axvline(x=0, color='k', linestyle='--', linewidth=0.8, label='Forecast Start (T=0)')
                plt.legend()
                plt.grid(True)
            plt.tight_layout()
            plt.savefig(f"forecast_examples_{SEASON.lower()}_patchtst_v3.png")

        # --- BENCHMARKING REPORT ---
        print("\n\n--- BENCHMARKING REPORT (V3) ---")
        total_params = solar_model.count_params()
        model_size_mb = total_params * 4 / (1024**2)

        print(f"{'Model Name:':<25} PatchTST ({SEASON} V3)")
        print(f"{'Key Features / Inputs:':<25} Renewable % (Univariate)")
        print("-" * 40)
        print(f"{'MAE:':<25} {mae_overall:.4f}")
        print(f"{'RMSE:':<25} {rmse_overall:.4f}")
        print(f"{'R²:':<25} {r2_overall:.4f}")
        print(f"{'Model Size (MB):':<25} {model_size_mb:.2f} MB")
        print(f"{'# of Params (M):':<25} {total_params / 1e6:.2f} M")
        print("-" * 40)
            
    else:
        print("No test data to evaluate or plot.")
else:
    print("CRITICAL: Not enough data to train the model.")
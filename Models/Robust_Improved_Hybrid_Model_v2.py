# Auto-generated from notebook by scripts/convert_notebooks.py
# Do not edit this file directly; edit the corresponding .ipynb instead.

#!/usr/bin/env python
# coding: utf-8

# # 72-Hour Renewable Energy Forecast (Robust Improved Hybrid V2) - Season-aware
# 
# Enhanced v2 builds upon the original hybrid by:
# - Adding lightweight positional encoding (sine/cosine) to help attention focus on relative time.
# - Introducing depthwise separable CNN residual blocks for richer local pattern extraction with fewer params.
# - Replacing BiLSTM(64->32) stack with a single Bidirectional LSTM + GRU fusion to reduce overfitting.
# - Multi-Head Attention replaced with two stacked attention blocks using smaller key_dim and pre-norm.
# - DLinear branch upgraded with dropout + linear trend detrending (subtract moving average) before projection.
# - Introduces gated mixture with temperature parameter and squeeze-excite recalibration.
# - Optional quantile loss (pinball) for probabilistic forecasting (median by default) while still reporting point metrics.
# 
# Workflow identical: params, data load, sequence build, model, train (early stopping), evaluation, plotting, benchmarking.
# 

# In[9]:


# --- Imports & Environment ---
import os, sys, math, random, json, glob


import numpy as np
import pandas as pd
import matplotlib
matplotlib.use("Agg")  # Use non-interactive backend to prevent GUI pop-ups
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.layers import (Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, Conv1D, DepthwiseConv1D,
                                     LSTM, GRU, Bidirectional, Add, Concatenate, GlobalAveragePooling1D, Lambda, Activation)
from tensorflow.keras.layers import Flatten, Reshape
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from scripts.season import resolve_season, months_for, data_path
SEASON = resolve_season(default='Winter').capitalize()
MONTHS = months_for(SEASON.lower())

SEED=42
random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)
print('TF', tf.__version__)

# Positional Encoding (sine/cosine)
def positional_encoding(length, depth):
    assert depth % 2 == 0
    positions = np.arange(length)[:, None]
    depths = np.arange(depth//2)[None, :]/(depth//2)
    angle_rates = 1 / (10000**depths)
    angle_rads = positions * angle_rates
    pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1)
    return tf.cast(pos_encoding, tf.float32)

# Pinball (Quantile) Loss
class QuantileLoss(tf.keras.losses.Loss):
    def __init__(self, q=0.5, name='quantile_loss'):
        super().__init__(name=name)
        self.q = q
    def call(self, y_true, y_pred):
        e = y_true - y_pred
        return tf.reduce_mean(tf.maximum(self.q*e, (self.q-1)*e))

# Squeeze-Excite for 1D latent vectors
from tensorflow.keras.layers import GlobalAveragePooling1D

def se_block(x, reduction=8):
    filters = int(x.shape[-1])
    se = GlobalAveragePooling1D()(x)  # (batch, channels)
    se = Dense(filters//reduction, activation='relu')(se)
    se = Dense(filters, activation='sigmoid')(se)
    se = Reshape((1, filters))(se)  # broadcast along time
    return x * se


# In[5]:


# --- Parameters ---
LOOK_BACK=72
FORECAST_HORIZON=72
N_FEATURES=1
COUNTRY_CODE = (os.environ.get("COUNTRY_CODE") or os.environ.get("BIONET_COUNTRY_CODE") or "DE").upper()
YEARS_HISTORY=5
DATA_FILENAME = str(data_path(f"energy_data_{COUNTRY_CODE}_{YEARS_HISTORY}years_{SEASON.lower()}.csv"))
USE_QUANTILE=False
QUANTILE=0.5

print('Data file expected:', DATA_FILENAME)

# Sequence builder

def create_sequences(values, look_back, horizon):
    X,y=[],[]
    if len(values) < look_back + horizon:
        return np.array([]), np.array([])
    for i in range(len(values)-look_back-horizon+1):
        X.append(values[i:i+look_back])
        y.append(values[i+look_back:i+look_back+horizon])
    return np.array(X), np.array(y)

# Load Data
print('Loading data...')
try:
    df = pd.read_csv(DATA_FILENAME, index_col=0, parse_dates=True)
    df = df[df.index.month.isin(MONTHS)]
    series = df.squeeze()
except FileNotFoundError:
    print('CRITICAL: file missing')
    series = pd.Series(dtype=float)

scaler = MinMaxScaler()
X_train=y_train=X_valid=y_valid=X_test=y_test=np.array([])
if not series.empty:
    scaled = scaler.fit_transform(series.values.reshape(-1,1)).flatten()
    X_seq, y_seq = create_sequences(scaled, LOOK_BACK, FORECAST_HORIZON)
    if X_seq.size>0:
        X_seq3 = X_seq.reshape((X_seq.shape[0], X_seq.shape[1], N_FEATURES))
        t_idx = int(len(X_seq3)*0.7); v_size=int(len(X_seq3)*0.15)
        X_train, y_train = X_seq3[:t_idx], y_seq[:t_idx]
        X_valid, y_valid = X_seq3[t_idx:t_idx+v_size], y_seq[t_idx:t_idx+v_size]
        X_test, y_test   = X_seq3[t_idx+v_size:], y_seq[t_idx+v_size:]

print('Shapes:', X_train.shape, X_valid.shape, X_test.shape)


# In[10]:


# --- Improved Hybrid V2 Model ---
from tensorflow.keras.layers import Dropout

def depthwise_res_block(x, filters, kernel_size=5, dropout=0.1):
    res = x
    x = DepthwiseConv1D(kernel_size, padding='same')(x)
    x = Conv1D(filters, 1, padding='same', activation='relu')(x)
    x = LayerNormalization(epsilon=1e-6)(x)
    x = Dropout(dropout)(x)
    if res.shape[-1] != filters:
        res = Conv1D(filters, 1, padding='same')(res)
    return Add()([res, x])

class MovingAverage(tf.keras.layers.Layer):
    def __init__(self, window=7):
        super().__init__(); self.window=window
    def call(self, x):
        kernel = tf.ones((self.window,1,1))/self.window
        x_padded = tf.pad(x, [[0,0],[self.window-1,0],[0,0]])
        ma = tf.nn.conv1d(x_padded, kernel, stride=1, padding='VALID')
        return ma

def build_hybrid_v2(input_shape, horizon, attn_heads=2, attn_dim=32, temperature=1.5):
    inp = Input(shape=input_shape)

    pos_const = positional_encoding(input_shape[0], attn_dim)
    def add_pos(x):
        pos_b = tf.broadcast_to(pos_const, [tf.shape(x)[0], tf.shape(x)[1], attn_dim])
        return tf.concat([x, pos_b], axis=-1)
    x_cat = Lambda(add_pos, name='add_positional')(inp)

    x = Conv1D(64,3,padding='causal',activation='relu')(x_cat)
    x = depthwise_res_block(x,64)
    x = depthwise_res_block(x,64)

    lstm_out = Bidirectional(LSTM(64, return_sequences=True))(x)  # shape (...,128)
    gru_out  = GRU(128, return_sequences=True)(x)                 # match 128
    seq_mix  = Add()([lstm_out, gru_out])

    att_in = LayerNormalization(epsilon=1e-6)(seq_mix)
    for _ in range(2):
        a_norm = LayerNormalization(epsilon=1e-6)(att_in)
        att = MultiHeadAttention(num_heads=attn_heads, key_dim=attn_dim, dropout=0.1)(a_norm, a_norm)
        att_in = Add()([att_in, att])
        ff = LayerNormalization(epsilon=1e-6)(att_in)
        ff = Dense(attn_dim*2, activation='relu')(ff)
        ff = Dropout(0.1)(ff)
        ff = Dense(att_in.shape[-1])(ff)
        att_in = Add()([att_in, ff])

    se_out = se_block(att_in)
    seq_rep = GlobalAveragePooling1D()(se_out)

    ma = MovingAverage(window=7)(inp)
    detrended = inp - ma
    flat = Flatten()(detrended)
    seasonal = Dense(horizon)(Dropout(0.1)(flat))
    trend    = Dense(horizon)(flat)
    linear_out = Add()([seasonal, trend])

    deep_latent = Dense(128, activation='relu')(seq_rep)
    deep_latent = Dropout(0.25)(deep_latent)
    deep_out = Dense(horizon)(deep_latent)

    gate_raw = Dense(horizon)(deep_latent)
    gate = Activation('sigmoid')(gate_raw / temperature)
    fused = gate * deep_out + (1-gate) * linear_out

    model = Model(inputs=inp, outputs=fused, name='RobustImprovedHybridV2')
    model.summary(line_length=150)
    return model

model_v2=None
if isinstance(X_train, np.ndarray) and X_train.size>0:
    model_v2 = build_hybrid_v2((LOOK_BACK, N_FEATURES), FORECAST_HORIZON)
else:
    print('Skipping build: no data.')


# In[11]:


# --- Training & Evaluation ---
history=None
if model_v2 is not None and X_train.size>0:
    loss_fn = QuantileLoss(QUANTILE) if USE_QUANTILE else 'mae'
    model_v2.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss=loss_fn, metrics=['mae'])
    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)
    history = model_v2.fit(
        X_train, y_train,
        validation_data=(X_valid, y_valid),
        epochs=500,
        batch_size=32,
        verbose=1,
        callbacks=[es]
    )

    # Plot
    plt.figure(figsize=(9,5))
    plt.plot(history.history['loss'], label='Train')
    plt.plot(history.history['val_loss'], label='Valid')
    plt.title(f'Loss ({SEASON}) - Hybrid V2')
    plt.xlabel('Epoch'); plt.ylabel('Loss')
    plt.grid(True); plt.legend(); plt.tight_layout()
    plt.savefig(f'training_loss_plot_{SEASON.lower()}_robust_improved_hybrid_v2.png')
    plt.close()

    if X_test.size>0:
        print('\n--- Test Evaluation ---')
        y_pred_scaled = model_v2.predict(X_test)
        y_test_flat = y_test.reshape(-1,1)
        y_pred_flat = y_pred_scaled.reshape(-1,1)
        y_test_inv = scaler.inverse_transform(y_test_flat)
        y_pred_inv = scaler.inverse_transform(y_pred_flat)
        # Expose inverse-scaled arrays for centralized metrics collection
        y_test_inversed = y_test_inv
        y_pred_inversed = y_pred_inv
        mae = mean_absolute_error(y_test_inv, y_pred_inv)
        mse = mean_squared_error(y_test_inv, y_pred_inv)
        rmse = math.sqrt(mse)
        r2 = r2_score(y_test_inv, y_pred_inv)
        print(f'MAE : {mae:.4f}\nRMSE: {rmse:.4f}\nR²  : {r2:.4f}')

        # Samples
        n_plot = min(3, len(X_test))
        if n_plot>0:
            plt.figure(figsize=(15,5*n_plot))
            for i in range(n_plot):
                idx = np.random.randint(0,len(X_test))
                hist_inv = scaler.inverse_transform(X_test[idx])
                true_inv = scaler.inverse_transform(y_test[idx].reshape(-1,1))
                pred_inv = scaler.inverse_transform(y_pred_scaled[idx].reshape(-1,1))
                t_in=np.arange(-LOOK_BACK,0); t_out=np.arange(0,FORECAST_HORIZON)
                plt.subplot(n_plot,1,i+1)
                plt.plot(t_in, hist_inv.flatten(), label='History', color='gray', linestyle=':')
                plt.plot(t_out, true_inv, label='Actual', color='blue')
                plt.plot(t_out, pred_inv, label='Predicted', color='red', linestyle='--')
                plt.axvline(0,color='k',linestyle='--',linewidth=0.8)
                plt.title(f'Sample idx={idx}')
                plt.xlabel('Hours'); plt.ylabel('Renewable %'); plt.legend(); plt.grid(True)
            plt.tight_layout(); plt.savefig(f'forecast_examples_{SEASON.lower()}_robust_improved_hybrid_v2.png'); plt.close()

        print('\n--- BENCHMARKING REPORT ---')
        total_params = model_v2.count_params(); size_mb = total_params*4/(1024**2)
        print(f"{'Model Name:':<25} RobustImprovedHybridV2 ({SEASON})")
        print(f"{'Key Inputs:':<25} Renewable % (Univariate)")
        print('-'*40)
        print(f"{'MAE:':<25} {mae:.4f}")
        print(f"{'RMSE:':<25} {rmse:.4f}")
        print(f"{'R²:':<25} {r2:.4f}")
        print(f"{'Model Size (MB):':<25} {size_mb:.2f} MB")
        print(f"{'# Params (M):':<25} {total_params/1e6:.2f} M")
        print('-'*40)
else:
    print('CRITICAL: Not enough data for training.')


# ### V2 Improvement Notes
# - Depthwise separable residual blocks reduce params vs stacked standard Conv1D.
# - Positional encoding helps model distinguish hour positions.
# - LSTM + GRU fusion broadens temporal dynamics capture.
# - Temperature-scaled gating prevents premature saturation of linear vs deep blend.
# - Optional quantile loss enables probabilistic forecasting; set USE_QUANTILE=True to activate.
# - Detrending via moving average encourages linear branch to model residual seasonal patterns.
# 
# Tuning Tips:
# - Increase attn_heads or attn_dim modestly for longer-range fidelity.
# - Lower temperature to make gating sharper; raise for smoother blending.
# - Adjust MovingAverage window based on dominant seasonality periods.
# 
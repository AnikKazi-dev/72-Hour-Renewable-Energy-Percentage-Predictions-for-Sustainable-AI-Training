# Auto-generated from notebook by scripts/convert_notebooks.py
# Do not edit this file directly; edit the corresponding .ipynb instead.

#!/usr/bin/env python
# coding: utf-8

# # 72-Hour Renewable Energy Forecast (Robust Improved Hybrid) - Season-aware Model with Benchmarking
# 
# This notebook programmatically inspects existing model notebooks (without modifying them) and builds an improved but still concise hybrid forecasting model inspired by patterns observed across Transformer, Autoformer, DLinear, CNN-LSTM, Ensemble, and PatchTST style architectures.
# 
# Key integrated ideas:
# - Local pattern extraction (1D Convolution)
# - Sequence memory (BiLSTM)
# - Long-range dependency (Multi-Head Attention with residuals)
# - Lightweight linear decomposition (DLinear-style seasonal + trend heads)
# - Ensembling via gated fusion (learned weights rather than naive averaging)
# 
# The workflow mirrors other model notebooks: parameters, data loading, sequence building, model creation, training with early stopping, evaluation, plotting, benchmarking.
# 

# In[5]:


# --- Imports & Environment Snapshot ---
import os, json, re, glob, random, math, sys


import numpy as np
import pandas as pd
import matplotlib
matplotlib.use("Agg")  # Use non-interactive backend to prevent GUI pop-ups
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.layers import (Input, Dense, Dropout, LayerNormalization, MultiHeadAttention,
                                     Conv1D, LSTM, Bidirectional, Add, Concatenate, GlobalAveragePooling1D)
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from scripts.season import resolve_season, months_for, data_path
SEASON = resolve_season(default='Winter').capitalize()
MONTHS = months_for(SEASON.lower())

# Reproducibility
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
try:
    tf.random.set_seed(SEED)
except Exception:
    pass

print(f"Python: {sys.version.split()[0]}")
print(f"TensorFlow: {tf.__version__}")
print(f"NumPy: {np.__version__}")
print(f"Pandas: {pd.__version__}")


# In[6]:


# --- Parameters ---
LOOK_BACK = 72
FORECAST_HORIZON = 72
N_FEATURES = 1
YEARS_HISTORY = 5
COUNTRY_CODE = (os.environ.get("COUNTRY_CODE") or os.environ.get("BIONET_COUNTRY_CODE") or "DE").upper()
DATA_FILENAME = str(data_path(f"energy_data_{COUNTRY_CODE}_{YEARS_HISTORY}years_{SEASON.lower()}.csv"))
print('Data file expected:', DATA_FILENAME)

# --- Utility: Create sequences ---
def create_sequences(scaled_values, look_back, horizon):
    X_list, y_list = [], []
    if len(scaled_values) < look_back + horizon:
        return np.array([]), np.array([])
    for i in range(len(scaled_values) - look_back - horizon + 1):
        X_list.append(scaled_values[i:i+look_back])
        y_list.append(scaled_values[i+look_back:i+look_back+horizon])
    return np.array(X_list), np.array(y_list)


# In[7]:


# --- Load & Filter Data (Season-aware) ---
print(f"Loading data from file: {DATA_FILENAME}")
try:
    cached_data = pd.read_csv(DATA_FILENAME, index_col=0, parse_dates=True)
    seasonal_data = cached_data[cached_data.index.month.isin(MONTHS)]
    renewable_series_data = seasonal_data.squeeze()
    print(f'{SEASON} data loaded:', len(renewable_series_data))
except FileNotFoundError:
    print('CRITICAL: data file missing')
    renewable_series_data = pd.Series(dtype=float)

scaler = MinMaxScaler(feature_range=(0,1))
X_train = y_train = X_valid = y_valid = X_test = y_test = np.array([])

if not renewable_series_data.empty:
    scaled = scaler.fit_transform(renewable_series_data.values.reshape(-1,1)).flatten()
    X_seq, y_seq = create_sequences(scaled, LOOK_BACK, FORECAST_HORIZON)
    if X_seq.size > 0:
        # 3D for hybrid model
        X_seq_3d = X_seq.reshape((X_seq.shape[0], X_seq.shape[1], N_FEATURES))
        train_idx = int(len(X_seq_3d)*0.70)
        valid_size = int(len(X_seq_3d)*0.15)
        X_train, y_train = X_seq_3d[:train_idx], y_seq[:train_idx]
        X_valid, y_valid = X_seq_3d[train_idx:train_idx+valid_size], y_seq[train_idx:train_idx+valid_size]
        X_test, y_test = X_seq_3d[train_idx+valid_size:], y_seq[train_idx+valid_size:]

print('\nData Split:')
print('X_train', getattr(X_train,'shape',None),' y_train', getattr(y_train,'shape',None))
print('X_valid', getattr(X_valid,'shape',None),' y_valid', getattr(y_valid,'shape',None))
print('X_test ', getattr(X_test,'shape',None),' y_test ', getattr(y_test,'shape',None))


# In[9]:


# --- Hybrid Model Definition ---

def build_hybrid_model(input_shape, horizon):
    inp = Input(shape=input_shape)

    # Branch 1: CNN -> BiLSTM
    x1 = Conv1D(64, 3, padding='causal', activation='relu')(inp)
    x1 = Conv1D(64, 3, padding='causal', activation='relu')(x1)
    x1 = Bidirectional(LSTM(64, return_sequences=True))(x1)
    x1 = Bidirectional(LSTM(32, return_sequences=False))(x1)

    # Branch 2: Attention stack (Transformer-lite)
    x2 = Conv1D(32, 1, activation='relu')(inp)
    for _ in range(2):
        norm = LayerNormalization(epsilon=1e-6)(x2)
        attn_out = MultiHeadAttention(num_heads=4, key_dim=32, dropout=0.1)(norm, norm)
        x2 = Add()([x2, attn_out])
        ff = LayerNormalization(epsilon=1e-6)(x2)
        ff = Dense(64, activation='relu')(ff)
        ff = Dropout(0.1)(ff)
        ff = Dense(x2.shape[-1])(ff)
        x2 = Add()([x2, ff])
    x2 = GlobalAveragePooling1D()(x2)

    # Branch 3: DLinear style (seasonal + trend linear projections) using Flatten layer
    from tensorflow.keras.layers import Flatten
    flat = Flatten()(inp)  # shape (batch, look_back * features)
    seasonal = Dense(horizon)(flat)
    trend = Dense(horizon)(flat)
    dlinear_out = Add()([seasonal, trend])

    # Fuse deep branches (x1, x2) into latent
    latent = Concatenate()([x1, x2])
    latent = Dense(128, activation='relu')(latent)
    latent = Dropout(0.2)(latent)
    deep_out = Dense(horizon)(latent)

    # Gated fusion with linear part
    gate = tf.keras.activations.sigmoid(Dense(horizon)(latent))
    fused = gate * deep_out + (1 - gate) * dlinear_out

    model = Model(inputs=inp, outputs=fused, name='RobustImprovedHybrid')
    model.summary(line_length=140)
    return model

solar_model = None
if isinstance(X_train, np.ndarray) and X_train.size > 0:
    solar_model = build_hybrid_model((LOOK_BACK, N_FEATURES), FORECAST_HORIZON)
else:
    print('Skipping model build: no training data.')


# In[10]:


# --- Training & Evaluation ---
history = None
if solar_model is not None and X_train.size > 0:
    solar_model.compile(optimizer='adam', loss='mae', metrics=['mae'])
    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)
    history = solar_model.fit(
        X_train, y_train,
        validation_data=(X_valid, y_valid),
        epochs=400,
        batch_size=32,
        verbose=1,
        callbacks=[early_stopping]
    )

    # Plot training curves
    plt.figure(figsize=(9,5))
    plt.plot(history.history['loss'], label='Train')
    plt.plot(history.history['val_loss'], label='Valid')
    plt.title(f'Training Loss ({SEASON}) - RobustImprovedHybrid')
    plt.xlabel('Epoch'); plt.ylabel('MAE')
    plt.grid(True); plt.legend()
    plt.savefig(f'training_loss_plot_{SEASON.lower()}_robust_improved_hybrid.png')
    plt.close()

    if X_test.size > 0:
        print('\n--- Evaluating on Test Set ---')
        y_pred_scaled = solar_model.predict(X_test)
        # Inverse transform
        y_test_flat = y_test.reshape(-1,1)
        y_pred_flat = y_pred_scaled.reshape(-1,1)
        y_test_inv = scaler.inverse_transform(y_test_flat)
        y_pred_inv = scaler.inverse_transform(y_pred_flat)
        # Expose inverse-scaled arrays for centralized metrics collection
        y_test_inversed = y_test_inv
        y_pred_inversed = y_pred_inv

        mae_overall = mean_absolute_error(y_test_inv, y_pred_inv)
        mse_overall = mean_squared_error(y_test_inv, y_pred_inv)
        rmse_overall = math.sqrt(mse_overall)
        r2_overall = r2_score(y_test_inv, y_pred_inv)

        print(f"MAE : {mae_overall:.4f}")
        print(f"RMSE: {rmse_overall:.4f}")
        print(f"R²  : {r2_overall:.4f}")

        # Plot a few sample forecasts
        num_plots = min(3, len(X_test))
        if num_plots > 0:
            plt.figure(figsize=(15,5*num_plots))
            for i in range(num_plots):
                idx = np.random.randint(0, len(X_test))
                hist_inv = scaler.inverse_transform(X_test[idx])
                true_inv = scaler.inverse_transform(y_test[idx].reshape(-1,1))
                pred_inv = scaler.inverse_transform(y_pred_scaled[idx].reshape(-1,1))
                t_in = np.arange(-LOOK_BACK,0)
                t_out = np.arange(0,FORECAST_HORIZON)
                plt.subplot(num_plots,1,i+1)
                plt.plot(t_in, hist_inv.flatten(), label='History', color='gray', linestyle=':')
                plt.plot(t_out, true_inv, label='Actual', color='blue')
                plt.plot(t_out, pred_inv, label='Predicted', color='red', linestyle='--')
                plt.axvline(0,color='k',linestyle='--',linewidth=0.8)
                plt.title(f'{FORECAST_HORIZON}-Hour Forecast Sample idx={idx}')
                plt.xlabel('Hours'); plt.ylabel('Renewable %')
                plt.legend(); plt.grid(True)
            plt.tight_layout()
            plt.savefig(f'forecast_examples_{SEASON.lower()}_robust_improved_hybrid.png')
            plt.close()

        # Benchmark report
    print('\n--- BENCHMARKING REPORT ---')
    total_params = solar_model.count_params()
    model_size_mb = total_params * 4 / (1024**2)
    print(f"{'Model Name:':<25} RobustImprovedHybrid ({SEASON})")
    print(f"{'Key Inputs:':<25} Renewable % (Univariate)")
    print('-'*40)
    print(f"{'MAE:':<25} {mae_overall:.4f}")
    print(f"{'RMSE:':<25} {rmse_overall:.4f}")
    print(f"{'R²:':<25} {r2_overall:.4f}")
    print(f"{'Model Size (MB):':<25} {model_size_mb:.2f} MB")
    print(f"{'# Params (M):':<25} {total_params/1e6:.2f} M")
    print('-'*40)
else:
    print('CRITICAL: Not enough data to train the model.')


# ### Notes
# - CNN layers capture short-term fluctuations.
# - BiLSTM layers model temporal order bidirectionally (improves over single LSTM seen in EnsembleCI).
# - Attention block adds global context similar to Transformer/Autoformer but kept shallow (2 layers) to stay lightweight.
# - DLinear-inspired dual linear heads supply a strong inductive bias for trend + seasonal components, blended by a sigmoid gate for adaptive weighting.
# - Gated fusion allows the model to lean on linear decomposition when deep branches overfit, improving robustness.
# 
# You can adjust capacity quickly:
# - Reduce Conv/LSTM units or attention heads for speed.
# - Increase latent size or add an attention layer for accuracy.
# 
# This notebook does not modify any original model files; it only reads shared data and builds a new architecture.
# 
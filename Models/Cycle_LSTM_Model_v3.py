# Auto-generated from notebook by scripts/convert_notebooks.py
# Do not edit this file directly; edit the corresponding .ipynb instead.

#!/usr/bin/env python
# coding: utf-8

# # 72-Hour Renewable Energy Forecast (Cycle-LSTM) - V3 Improved Model
# 
# ## Modifications for V3:
# 
# This notebook enhances the V2 Cycle-LSTM model with several key improvements aimed at boosting predictive accuracy and ensuring robustness:
# 
# 1.  **Huber Loss Function**: The `Mean Absolute Error` loss function has been replaced with the `Huber` loss. The Huber loss is a robust loss function that is less sensitive to outliers in the data than `Mean Squared Error` and can provide a good balance between `MAE` and `RMSE`. This should help in training a more stable model, especially if there are anomalies in the energy data.
# 
# 2.  **L2 Regularization**: L2 regularization has been added to the `LSTM` and `Dense` layers in the model. L2 regularization, also known as weight decay, penalizes large weights in the model. This helps to prevent overfitting by encouraging the model to learn smaller and more distributed weights, which can lead to better generalization on unseen data.
# 
# 3.  **Increased Model Capacity**: To potentially capture more intricate patterns in the data, the model's complexity has been increased. The number of units in the `LSTM` layers is increased from `64` to `128`, and the `Dense` layer units from `100` to `128`. This gives the model more capacity to learn from the time series data.
# 
# 4.  **Batch Normalization**: A `BatchNormalization` layer has been added after the main `Dense` layer. This helps to stabilize and potentially speed up the training process by normalizing the inputs to the next layer.

# In[1]:


import pandas as pd


import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout, LSTM, Bidirectional, BatchNormalization
from tensorflow.keras.callbacks import ReduceLROnPlateau
from datetime import datetime, timedelta
import os
from sklearn.metrics import r2_score
from scripts.season import resolve_season, months_for, data_path
SEASON = resolve_season(default='Winter').capitalize()
MONTHS = months_for(SEASON.lower())

print(f"TensorFlow Version: {tf.__version__}")


# In[2]:


# --- Data Parameters ---
LOOK_BACK = 72          # Use past 72 hours (3 days) of data to predict
FORECAST_HORIZON = 72   # Predict next 72 hours
N_FEATURES = 1          # We are using only 'renewable_percentage' as the input feature
COUNTRY_CODE = (os.environ.get("COUNTRY_CODE") or os.environ.get("BIONET_COUNTRY_CODE") or "DE").upper()
YEARS_HISTORY = 5       # Set history to 5 years

# --- Seasonal Parameters ---
DATA_FILENAME = str(data_path(f"energy_data_{COUNTRY_CODE}_{YEARS_HISTORY}years_{SEASON.lower()}.csv"))


# In[3]:


def create_sequences(data_values_scaled, look_back, forecast_horizon):
    """Creates sequences of X (input) and y (target) for time series forecasting."""
    X_list, y_list = [], []
    if len(data_values_scaled) < look_back + forecast_horizon:
        print(f"Not enough data to create sequences. Data length: {len(data_values_scaled)}, "
              f"Required: {look_back + forecast_horizon}")
        return np.array(X_list), np.array(y_list)
        
    for i in range(len(data_values_scaled) - look_back - forecast_horizon + 1):
        X_list.append(data_values_scaled[i:(i + look_back)])
        y_list.append(data_values_scaled[(i + look_back):(i + look_back + forecast_horizon)])
    return np.array(X_list), np.array(y_list)


# In[4]:


# --- Load and filter data from the season-specific CSV file ---
print(f"Loading data from file: {DATA_FILENAME}")
try:
    cached_data = pd.read_csv(DATA_FILENAME, index_col=0, parse_dates=True)
    # Filter the loaded data to ensure it only contains months for the selected season
    season_data = cached_data[cached_data.index.month.isin(MONTHS)]
    renewable_series_data = season_data.squeeze()
    print("Data loaded and filtered successfully.")
except FileNotFoundError:
    print(f"CRITICAL: Data file not found at '{os.path.abspath(DATA_FILENAME)}'.")
    print(f"Please ensure the {SEASON.lower()} CSV file exists.")
    renewable_series_data = None

# --- The rest of the cell proceeds from here ---
X_train, y_train = np.array([]), np.array([])
X_valid, y_valid = np.array([]), np.array([])
X_test, y_test = np.array([]), np.array([])
scaler = MinMaxScaler(feature_range=(0, 1)) # Initialize scaler here for later use

if renewable_series_data is not None and not renewable_series_data.empty:
    print(f"\nOriginal data points loaded: {len(renewable_series_data)}")
    # Scale the data
    data_for_scaling = renewable_series_data.values.reshape(-1, 1)
    scaled_data_values = scaler.fit_transform(data_for_scaling).flatten()

    # Create sequences
    X_seq, y_seq = create_sequences(scaled_data_values, LOOK_BACK, FORECAST_HORIZON)

    if X_seq.shape[0] > 0:
        X_seq = X_seq.reshape((X_seq.shape[0], X_seq.shape[1], N_FEATURES))

        # Chronological split for time series
        train_size_idx = int(len(X_seq) * 0.70)
        valid_size_idx = int(len(X_seq) * 0.15)

        X_train, y_train = X_seq[:train_size_idx], y_seq[:train_size_idx]
        X_valid, y_valid = X_seq[train_size_idx : train_size_idx + valid_size_idx], y_seq[train_size_idx : train_size_idx + valid_size_idx]
        X_test, y_test = X_seq[train_size_idx + valid_size_idx:], y_seq[train_size_idx + valid_size_idx:]
else:
    print("CRITICAL: No data loaded or data is empty. Cannot proceed with model training.")

print(f"\nData Split:")
print(f"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")
print(f"X_valid shape: {X_valid.shape}, y_valid shape: {y_valid.shape}")
print(f"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}")


# In[5]:


def build_cycle_lstm_model(input_shape, forecast_horizon):
    inputs = Input(shape=input_shape)
    
    # V3: Increased capacity and L2 regularization
    x = Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(0.001)))(inputs)
    x = Bidirectional(LSTM(128, return_sequences=False, kernel_regularizer=tf.keras.regularizers.l2(0.001)))(x)
    
    # Dense layers
    x = Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x) # V3: Increased capacity and L2 regularization
    x = BatchNormalization()(x) # V3: Added Batch Normalization
    x = Dropout(0.3)(x)
    outputs = Dense(forecast_horizon)(x)
    
    model = Model(inputs=inputs, outputs=outputs)
    model.summary(line_length=120)
    return model

# Build the model
solar_model = None
if X_train.shape[0] > 0:
    solar_model = build_cycle_lstm_model(
        input_shape=(LOOK_BACK, N_FEATURES),
        forecast_horizon=FORECAST_HORIZON
    )
else:
    print("Skipping model building as no training data is available.")


# In[ ]:


history = None
if solar_model and X_train.shape[0] > 0:
    print("\n--- Starting Model Training ---")
    
    # V3: Use Huber loss
    loss = tf.keras.losses.Huber()
    solar_model.compile(optimizer='adam', loss=loss, metrics=['mae'])
    
    early_stopping = tf.keras.callbacks.EarlyStopping(
        monitor='val_loss', 
        patience=30,
        restore_best_weights=True
    )
    
    lr_scheduler = ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.2,
        patience=5,
        min_lr=1e-6,
        verbose=1
    )
    
    history = solar_model.fit(X_train, y_train,
                              validation_data=(X_valid, y_valid),
                              epochs=500,
                              batch_size=32,
                              verbose=1,
                              callbacks=[early_stopping, lr_scheduler])

    # --- Plotting and Evaluation ---
    from sklearn.metrics import mean_absolute_error, mean_squared_error

    # Plotting Training Loss
    plt.figure(figsize=(10, 6))
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title(f'Model Loss During Training ({SEASON}) - Cycle-LSTM V3')
    plt.ylabel('Loss (Huber)')
    plt.xlabel('Epoch')
    plt.legend(loc='upper right')
    plt.grid(True)
    plt.savefig(f"training_loss_plot_{SEASON.lower()}_cycle_lstm_v3.png")

    # Final Model Evaluation
    if X_test.shape[0] > 0:
        print("\n--- Evaluating Model on Test Set ---")
        y_pred_scaled = solar_model.predict(X_test)
        
        y_test_reshaped = y_test.reshape(-1, 1)
        y_pred_reshaped = y_pred_scaled.reshape(-1, 1)

        y_test_inversed = scaler.inverse_transform(y_test_reshaped)
        y_pred_inversed = scaler.inverse_transform(y_pred_reshaped)

        mae_overall = mean_absolute_error(y_test_inversed, y_pred_inversed)
        mse_overall = mean_squared_error(y_test_inversed, y_pred_inversed)
        rmse_overall = np.sqrt(mse_overall)
        r2_overall = r2_score(y_test_inversed, y_pred_inversed)

        print(f"\nOverall Test Set Metrics (on inverse-transformed data):")
        print(f"  Mean Absolute Error (MAE): {mae_overall:.4f}")
        print(f"  Mean Squared Error (MSE):  {mse_overall:.4f}")
        print(f"  Root Mean Squared Error (RMSE): {rmse_overall:.4f}")
        print(f"  R-squared (R²): {r2_overall:.4f}")

        # Make a Sample Prediction and Plot
        num_plots = min(3, len(X_test))
        if num_plots > 0:
            plt.figure(figsize=(15, 5 * num_plots))
            for i in range(num_plots):
                sample_index = np.random.randint(0, len(X_test))
                
                historical_input_inversed = scaler.inverse_transform(X_test[sample_index])
                y_true_inversed_plot = scaler.inverse_transform(y_test[sample_index].reshape(-1, 1))
                y_pred_inversed_plot = scaler.inverse_transform(y_pred_scaled[sample_index].reshape(-1, 1))

                time_axis_input = np.arange(-LOOK_BACK, 0)
                time_axis_output = np.arange(0, FORECAST_HORIZON)
                
                plt.subplot(num_plots, 1, i + 1)
                plt.plot(time_axis_input, historical_input_inversed.flatten(), label='Historical Input', marker='o', linestyle=':', color='gray', alpha=0.7)
                plt.plot(time_axis_output, y_true_inversed_plot, label='Actual Future', marker='.', color='blue')
                plt.plot(time_axis_output, y_pred_inversed_plot, label='Predicted Future', marker='x', linestyle='--', color='red')
                plt.title(f'{FORECAST_HORIZON}-Hour Forecast ({SEASON} - Test Sample {sample_index}) - V3')
                plt.xlabel('Time (Hours into the future)')
                plt.ylabel('Renewable Percentage (%)')
                plt.axvline(x=0, color='k', linestyle='--', linewidth=0.8, label='Forecast Start (T=0)')
                plt.legend()
                plt.grid(True)
            plt.tight_layout()
            plt.savefig(f"forecast_examples_{SEASON.lower()}_cycle_lstm_v3.png")

        # --- BENCHMARKING REPORT ---
        print("\n\n--- BENCHMARKING REPORT (V3) ---")
        total_params = solar_model.count_params()
        model_size_mb = total_params * 4 / (1024**2)

        print(f"{'Model Name:':<25} Cycle-LSTM ({SEASON} V3)")
        print(f"{'Key Features / Inputs:':<25} Renewable % (Univariate)")
        print("-" * 40)
        print(f"{'MAE:':<25} {mae_overall:.4f}")
        print(f"{'RMSE:':<25} {rmse_overall:.4f}")
        print(f"{'R²:':<25} {r2_overall:.4f}")
        print(f"{'Model Size (MB):':<25} {model_size_mb:.2f} MB")
        print(f"{'# of Params (M):':<25} {total_params / 1e6:.2f} M")
        print("-" * 40)
            
    else:
        print("No test data to evaluate or plot.")
else:
    print("CRITICAL: Not enough data to train the model.")
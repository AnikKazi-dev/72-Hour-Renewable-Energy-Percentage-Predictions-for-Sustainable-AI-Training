# Model Descriptions

Below is a tabular summary of the 14 model families in this project, the versions present in the repository, and a concise description (3–4 lines) that covers the intent and version evolution of each family.

| Model family                      | Versions (files)                                                                                                                | Description                                                                                                                                                                                                                                                                                                                |
| --------------------------------- | ------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Autoformer                        | Autoformer_Model.py, Autoformer_Model_v2.py, Autoformer_Model_v3.py                                                             | Long-horizon time-series model using auto-correlation blocks to capture periodic patterns and long-term dependencies. Decomposes series into trend/seasonality for efficient forecasting. v2 increases capacity/regularization for stability; v3 deepens stacks and refines hyperparameters for improved seasonal capture. |
| CarbonCast                        | CarbonCast_Model.py, CarbonCast_Model_v2.py, CarbonCast_Model_v3.py                                                             | Transformer-style forecaster tailored to carbon/renewables signals (embedding + attention + projection). Emphasizes temporal attention to recent and periodic contexts. v2 strengthens feature dropout/normalization and head widths; v3 scales depth/hidden size for harder regimes while guarding overfit.               |
| CNN + LSTM                        | CNN_LSTM_Model.py, CNN_LSTM_Model_v2.py, CNN_LSTM_Model_v3.py                                                                   | Hybrid extractor where 1D CNN captures short-term local patterns and LSTM models longer temporal context. Suitable for univariate renewable% with sequence-to-sequence heads. v2 adds filters/dropout and tuning; v3 introduces residual skips and attention-style gating for sharper forecasts.                           |
| Cycle LSTM                        | Cycle_LSTM_Model.py, Cycle_LSTM_Model_v2.py, Cycle_LSTM_Model_v3.py                                                             | LSTM-based sequence model biased toward cyclical/seasonal structures (daily/weekly). Multi-horizon output with sliding windows. v2 expands multi-scale windows and hidden size; v3 adds residual connections/regularization to stabilize longer lookbacks.                                                                 |
| DLinear                           | DLinear_Model.py, DLinear_Model_v2.py, DLinear_Model_v3.py                                                                      | Lightweight linear forecaster with trend/seasonal decomposition and channel-wise linear heads. Strong baseline for long series without heavy attention. v2 increases channels and normalization; v3 refines decomposition and regularization for robust season split performance.                                          |
| EnsembleCI                        | EnsembleCI_Model.py, EnsembleCI_Model_v2.py, EnsembleCI_Model_v3.py                                                             | Ensemble over diverse base learners to improve stability across countries/seasons. Aggregates predictions to reduce variance. v2 moves to learned/weighted averaging; v3 introduces a simple meta-learner (stacking) and calibrated uncertainty for selection.                                                             |
| Hybrid CNN–CycleLSTM + Attention  | Hybrid_CNN_CycleLSTM_Attention_Model.py, Hybrid_CNN_CycleLSTM_Attention_Model_v2.py, Hybrid_CNN_CycleLSTM_Attention_Model_v3.py | Combines CNN (local features) with CycleLSTM (seasonality) and attention heads for long-range dependencies. Targets multi-step univariate forecasting. v2 widens CNN/LSTM layers and dropout; v3 upgrades attention (multi-head) and residualization for deeper horizons.                                                  |
| Informer                          | Informer_Model.py, Informer_Model_v2.py, Informer_Model_v3.py                                                                   | Efficient long-sequence transformer using ProbSparse attention and distillation for reduced complexity. Strong at capturing periodic patterns with long contexts. v2 increases encoder depth/heads; v3 tunes patching/distillation and adds stronger regularization.                                                       |
| Mamba                             | Mamba_Model.py, Mamba_Model_v2.py, Mamba_Model_v3.py                                                                            | Modern sequence architecture inspired by state-space/selective scan methods for efficient long-range modeling. Suited to long, hourly series with stable gradients. v2 scales hidden/state size and normalization; v3 deepens layers and adds dropout schedule for robustness.                                             |
| N-BEATS                           | N_Beats_Model.py, N_Beats_Model_v2.py, N_Beats_Model_v3.py                                                                      | Backcast/forecast residual stacks with trend/seasonality basis expansions; strong classical-deep hybrid baseline. Interpretable blocks capture components explicitly. v2 increases block count/width; v3 adds improved basis and regularization for season-specific accuracy.                                              |
| PatchTST                          | PatchTST_Model.py, PatchTST_Model_v2.py, PatchTST_Model_v3.py                                                                   | Transformer over patched time segments (patch embedding + positional encoding) enabling efficient tokenization of time windows. Good for long sequences with local/global mixing. v2 scales patch size/heads and dropout; v3 deepens layers and adds GELU/projection tweaks.                                               |
| Robust Improved Hybrid            | Robust_Improved_Hybrid_Model.py, Robust_Improved_Hybrid_Model_v2.py                                                             | Robust hybrid forecaster combining complementary modules (e.g., CNN/LSTM/attention) with emphasis on stability and ordering of preprocessing. Only two versions present. v2 improves variable ordering, normalization, and training stability; no v3 in repo.                                                              |
| Temporal Fusion Transformer (TFT) | Temporal_Fusion_Transformer_Model.py, Temporal_Fusion_Transformer_Model_v2.py, Temporal_Fusion_Transformer_Model_v3.py          | Interpretable transformer with gating, variable selection, and attention for multi-horizon forecasting. Handles temporal dynamics and feature importance. v2 increases hidden sizes/heads and adds dropout; v3 deepens fully connected layers and refines gating/normalization.                                            |
| Transformer                       | Transformer_Model.py, Transformer_Model_v2.py, Transformer_Model_v3.py                                                          | Baseline transformer for time series (token/positional embeddings + MH attention + FFN). Encoder-style architectures for sequence-to-sequence outputs. v2 adds richer token/positional embeddings and model width; v3 significantly scales depth/hidden size and regularization for accuracy.                              |

Notes:

- COUNTRY_CODE and season-aware data loaders are standardized across all families; descriptions focus on modeling differences.
- “v2” typically introduces modest capacity/regularization changes; “v3” generally scales depth/width and refines training.
- Robust_Improved_Hybrid is the only family with two versions in the repository.
